#Структура перцептрона с алгоритмом обратного распространения ошибки
# X (layer_0) - layer_1 - Y.
# X (layer_0)- выходные данные, передаются на скрытый слой layer_1
# после на выход Y. Выходное значение сравнивается
# с требеумыми значениями и формируется величина ошибки для коррекции весов

#Библиотеки
import numpy as np
#Переменные
num_iterations = 1000 # Количество итераций обучения
bias = 2 # Величина смещения

# Сигмоида
def sigmoid(x, deriv = False):
    if (deriv == True): #Производная сигмоиды
        return x * (1 - x)
    return 1 / (1 + np.exp(-x))

# набор входных данных
x = np.array([[0, 0, 1],
              [0, 1, 1],
              [1, 0, 1],
              [1, 1, 1]])

# выходные данные
y = np.array([[0, 0, 1, 1]]).T # Транспонируем матрицу
np.random.seed(1) #Сделаем случайные числа более определёнными

# syn0 – первый слой весов, Synapse 0, объединяет layer0 с layer1
syn0 = 2 * np.random.random((3, 1)) - bias # Синапс 0
print("Синапс 0 до обучения", "\n", syn0)

for iter in range(num_iterations):
    layer_0 = x # layer_0 – первый слой сети, определённый входными данными
    #layer_1 – второй слой сети, или скрытый слой
    layer_1 = sigmoid(np.dot(layer_0, syn0)) #Функция dot() вычисляет скалярное произведение двух массивов
    l1_error = y - layer_1 #НВычисляем величину ошибки
    l1_delta = l1_error * sigmoid(layer_1, True) # перемножим это с наклоном сигмоиды, на основе значений в layer_1
    syn0 += np.dot(layer_0.T, l1_delta)   # обновим веса

print("\n Выходные данные после тренировки:", "\n", layer_1)
print("\n При заданных значениях: ""\n", y)
print("\n Синапс 0 после обучения", "\n", syn0)